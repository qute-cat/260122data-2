# ============================================================
# AI Agents íŠ¸ë Œë“œ Ã— ì§„ë¡œêµìœ¡(ì‚¬ê³  í™•ì¥) Streamlit App
# - CSV(Title, Source, Date, Description, Link) ê¸°ë°˜
# - ë¶„ì„(ë³€í™”/í‚¤ì›Œë“œ/ì—­í• /ê²½ë¡œ) â†’ ìê¸°í™”(ì¤€ë¹„ ë¡œë“œë§µ) ì‚°ì¶œë¬¼
# ============================================================

import os
import re
from datetime import date, timedelta
from collections import Counter

import streamlit as st
import pandas as pd
import plotly.express as px


# -------------------------------
# Page config
# -------------------------------
st.set_page_config(page_title="AI Agents íŠ¸ë Œë“œ Ã— ì§„ë¡œêµìœ¡", layout="wide")


# -------------------------------
# Constants
# -------------------------------
DEFAULT_PATH = "AI_Agents_Ecosystem_2026.csv"
DATA_PATH = os.getenv("AI_AGENT_CSV_PATH", DEFAULT_PATH)

STOP_EN = {
    "the","and","with","for","from","this","that","into","onto","over","under","about","between",
    "using","use","used","new","latest","toward","towards","via","based","approach","system","systems",
    "paper","research","study","studies","results","method","methods","model","models","dataset","data",
    "ai","agent","agents","llm","llms","gpt","openai","anthropic","google","meta","microsoft",
    "framework","tool","tools","application","applications","analysis","report","reports",
    "build","building","improve","improving","improved","evaluate","evaluation","evaluating","benchmark",
    "release","released","update","updated","updates","today","yesterday","tomorrow"
}

ROLE_DEFS = [
    ("ì„¤ê³„ì(ê¸°íš/êµ¬ì¡°í™”/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)", [
        r"\borchestrat", r"\bworkflow", r"\bpipeline", r"\bplanner", r"\bplanning",
        r"\barchitecture", r"\bdesign", r"\brouter", r"\bcoordinator", r"\bprompt\s*design"
    ]),
    ("êµ¬í˜„ì(ê°œë°œ/ìë™í™”)", [
        r"\bimplement", r"\bimplementation", r"\bbuild", r"\bdev", r"\bdeveloper",
        r"\bcode", r"\blibrary", r"\bsdk\b", r"\bapi\b", r"\bintegration", r"\bplugin",
        r"\bgithub\b", r"\btypescript\b", r"\bpython\b", r"\bnode\b"
    ]),
    ("ìš´ì˜ì(ë°°í¬/ëª¨ë‹ˆí„°ë§/MLOps)", [
        r"\bdeploy", r"\bdeployment", r"\bops\b", r"\bmlops\b", r"\bmonitor",
        r"\bobservability", r"\bproduction", r"\breliability", r"\binfra", r"\bkubernetes",
        r"\bserver", r"\bscaling", r"\blatency"
    ]),
    ("ë¶„ì„ê°€(ë¦¬ì„œì¹˜/ë°ì´í„°)", [
        r"\barxiv\b", r"\bpaper\b", r"\bstudy\b", r"\bdata\b", r"\bdataset\b",
        r"\bstat", r"\bempirical", r"\bexperiment", r"\bmethodology", r"\btheory",
        r"\bsurvey\b"
    ]),
    ("í‰ê°€ì(Eval/ê²€ì¦/ì•ˆì „)", [
        r"\beval", r"\bevaluation", r"\bbenchmark", r"\btest", r"\btesting",
        r"\bverification", r"\bvalidat", r"\bsafety", r"\balignment", r"\brisk",
        r"\bguardrail", r"\bpolicy"
    ]),
    ("ì»¤ë®¤ë‹ˆì¼€ì´í„°(êµìœ¡/PM/ë²ˆì—­)", [
        r"\bguide\b", r"\btutorial", r"\bexplainer", r"\bdocument", r"\bdocumentation",
        r"\bcommunity", r"\bproduct", r"\bpm\b", r"\bteaching", r"\bcourse", r"\bwriting"
    ]),
]

SKILL_TECH = ["Python", "API/ì—°ë™", "ë°ì´í„° ì²˜ë¦¬", "LLM/RAG", "ì—ì´ì „íŠ¸/ì›Œí¬í”Œë¡œìš°", "í´ë¼ìš°ë“œ/ë°°í¬", "ë³´ì•ˆ/ìœ¤ë¦¬"]
SKILL_COG  = ["ë¬¸ì œì •ì˜", "êµ¬ì¡°í™”", "ì‹¤í—˜/ê²€ì¦", "ë…¼ë¦¬ì  ê¸€ì“°ê¸°", "ëª¨ë¸ë§/ì¶”ë¡ ", "ì •ë³´íƒìƒ‰", "ì‹œìŠ¤í…œ ì‚¬ê³ "]
SKILL_ATT  = ["ìê¸°ì£¼ë„", "í˜‘ì—…", "ë¶ˆí™•ì‹¤ì„± ê°ë‚´", "í•™ìŠµ ë¯¼ì²©ì„±", "ì±…ì„ê°", "ì‚¬ìš©ì ê´€ì ", "ëˆê¸°"]

ROLE_TO_SKILLS = {
    "ì„¤ê³„ì(ê¸°íš/êµ¬ì¡°í™”/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)": {
        "tech": ["API/ì—°ë™", "ì—ì´ì „íŠ¸/ì›Œí¬í”Œë¡œìš°", "LLM/RAG"],
        "cog": ["ë¬¸ì œì •ì˜", "êµ¬ì¡°í™”", "ì‹œìŠ¤í…œ ì‚¬ê³ ", "ì •ë³´íƒìƒ‰"],
        "att": ["ì‚¬ìš©ì ê´€ì ", "í˜‘ì—…", "í•™ìŠµ ë¯¼ì²©ì„±"]
    },
    "êµ¬í˜„ì(ê°œë°œ/ìë™í™”)": {
        "tech": ["Python", "API/ì—°ë™", "ë°ì´í„° ì²˜ë¦¬", "ì—ì´ì „íŠ¸/ì›Œí¬í”Œë¡œìš°"],
        "cog": ["êµ¬ì¡°í™”", "ë¬¸ì œì •ì˜", "ì •ë³´íƒìƒ‰"],
        "att": ["ìê¸°ì£¼ë„", "ëˆê¸°", "ì±…ì„ê°"]
    },
    "ìš´ì˜ì(ë°°í¬/ëª¨ë‹ˆí„°ë§/MLOps)": {
        "tech": ["í´ë¼ìš°ë“œ/ë°°í¬", "API/ì—°ë™", "ë³´ì•ˆ/ìœ¤ë¦¬"],
        "cog": ["ì‹œìŠ¤í…œ ì‚¬ê³ ", "ì‹¤í—˜/ê²€ì¦", "ë¬¸ì œì •ì˜"],
        "att": ["ì±…ì„ê°", "ë¶ˆí™•ì‹¤ì„± ê°ë‚´", "í˜‘ì—…"]
    },
    "ë¶„ì„ê°€(ë¦¬ì„œì¹˜/ë°ì´í„°)": {
        "tech": ["ë°ì´í„° ì²˜ë¦¬", "Python", "LLM/RAG"],
        "cog": ["ì‹¤í—˜/ê²€ì¦", "ë…¼ë¦¬ì  ê¸€ì“°ê¸°", "ì •ë³´íƒìƒ‰", "ëª¨ë¸ë§/ì¶”ë¡ "],
        "att": ["í•™ìŠµ ë¯¼ì²©ì„±", "ëˆê¸°", "ìê¸°ì£¼ë„"]
    },
    "í‰ê°€ì(Eval/ê²€ì¦/ì•ˆì „)": {
        "tech": ["ë³´ì•ˆ/ìœ¤ë¦¬", "ë°ì´í„° ì²˜ë¦¬", "LLM/RAG"],
        "cog": ["ì‹¤í—˜/ê²€ì¦", "ë¬¸ì œì •ì˜", "ë…¼ë¦¬ì  ê¸€ì“°ê¸°"],
        "att": ["ì±…ì„ê°", "ë¶ˆí™•ì‹¤ì„± ê°ë‚´", "ì‚¬ìš©ì ê´€ì "]
    },
    "ì»¤ë®¤ë‹ˆì¼€ì´í„°(êµìœ¡/PM/ë²ˆì—­)": {
        "tech": ["API/ì—°ë™", "LLM/RAG"],
        "cog": ["ë…¼ë¦¬ì  ê¸€ì“°ê¸°", "ë¬¸ì œì •ì˜", "ì •ë³´íƒìƒ‰"],
        "att": ["í˜‘ì—…", "ì‚¬ìš©ì ê´€ì ", "ì±…ì„ê°"]
    },
}


# -------------------------------
# Helpers
# -------------------------------
@st.cache_data(show_spinner=False)
def read_csv_safely(path: str) -> pd.DataFrame:
    # ì¸ì½”ë”© fallback (utf-8 â†’ cp949 â†’ latin1)
    last_err = None
    for enc in ("utf-8", "cp949", "latin1"):
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception as e:
            last_err = e
    raise last_err


@st.cache_data(show_spinner=False)
def load_data(path: str) -> pd.DataFrame:
    df = read_csv_safely(path)

    # normalize columns (case-insensitive)
    cols = {c.strip().lower(): c for c in df.columns}
    need = ["title", "source", "date", "description", "link"]
    missing = [n for n in need if n not in cols]
    if missing:
        raise ValueError(f"CSV ì»¬ëŸ¼ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. í•„ìš”í•œ ì»¬ëŸ¼: {', '.join([n.title() for n in need])}")

    df = df.rename(columns={
        cols["title"]: "title",
        cols["source"]: "source",
        cols["date"]: "date",
        cols["description"]: "desc",
        cols["link"]: "link",
    })

    # sanitize types
    for c in ["title", "source", "desc", "link"]:
        df[c] = df[c].astype(str).fillna("").str.strip()

    # parse date
    df["date"] = pd.to_datetime(df["date"], errors="coerce")

    # domain
    df["domain"] = df["link"].str.extract(r"https?://([^/]+)", expand=False).fillna("")

    # content type (source-based)
    s = df["source"].str.lower()
    df["content_type"] = "news"
    df.loc[s.str.contains("arxiv"), "content_type"] = "paper"
    df.loc[s.str.contains("job"), "content_type"] = "job"

    # role classification (rule-based)
    def classify_role(title: str, desc: str, source: str) -> str:
        text = f"{title} {desc} {source}".lower()
        for role, patterns in ROLE_DEFS:
            for p in patterns:
                if re.search(p, text):
                    return role
        # fallback: content type cues
        if "arxiv" in text:
            return "ë¶„ì„ê°€(ë¦¬ì„œì¹˜/ë°ì´í„°)"
        return "ì„¤ê³„ì(ê¸°íš/êµ¬ì¡°í™”/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)"

    df["role"] = [classify_role(t, d, s) for t, d, s in zip(df["title"], df["desc"], df["source"])]

    # drop empties + dedupe
    df = df[(df["title"] != "") & (df["link"] != "")]
    df = df.drop_duplicates(subset=["link"]).reset_index(drop=True)

    # month/week for trends
    if df["date"].notna().any():
        df["month"] = df["date"].dt.to_period("M").astype(str)
        df["week"] = df["date"].dt.to_period("W").astype(str)
    else:
        df["month"] = ""
        df["week"] = ""

    return df


def tokenize_en(text: str):
    tokens = re.findall(r"[A-Za-z][A-Za-z0-9\-\+]{2,}", str(text).lower())
    tokens = [t for t in tokens if t not in STOP_EN]
    return tokens


def top_keywords(df: pd.DataFrame, n=25):
    tokens = []
    for t, d in zip(df["title"], df["desc"]):
        tokens.extend(tokenize_en(f"{t} {d}"))
    c = Counter(tokens)
    return c.most_common(n)


def rising_keywords(df_all: pd.DataFrame, recent_days: int = 30, n=15):
    if not df_all["date"].notna().any():
        return []

    cutoff = pd.Timestamp(date.today() - timedelta(days=recent_days))
    recent = df_all[df_all["date"] >= cutoff]
    if len(recent) == 0:
        return []

    all_counts = Counter(tokenize_en(" ".join((df_all["title"] + " " + df_all["desc"]).tolist())))
    recent_counts = Counter(tokenize_en(" ".join((recent["title"] + " " + recent["desc"]).tolist())))

    # score = recent frequency normalized - overall frequency normalized
    all_total = sum(all_counts.values()) or 1
    recent_total = sum(recent_counts.values()) or 1

    scores = []
    for k, rc in recent_counts.items():
        ac = all_counts.get(k, 0)
        score = (rc / recent_total) - (ac / all_total)
        scores.append((k, score, rc, ac))

    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:n]


def item_label(row: pd.Series) -> str:
    d = row["date"].date().isoformat() if pd.notna(row["date"]) else ""
    return f"[{row['content_type']}] {d} Â· {row['title'][:95]}"


def triad_pick(df: pd.DataFrame, keyword: str):
    key = keyword.lower().strip()
    sub = df[
        df["title"].str.lower().str.contains(key, na=False) |
        df["desc"].str.lower().str.contains(key, na=False)
    ].copy()
    picks = {}
    for t in ["paper", "news", "job"]:
        tmp = sub[sub["content_type"] == t].sort_values("date", ascending=False)
        if len(tmp) > 0:
            picks[t] = tmp.iloc[0]
    return picks, sub


# -------------------------------
# Session state
# -------------------------------
st.session_state.setdefault("portfolio", [])     # ì„ íƒ/ê¸°ë¡ ëˆ„ì 
st.session_state.setdefault("notes", [])         # ì¶”ê°€ ë©”ëª¨
st.session_state.setdefault("student_questions", [])  # ìµëª… ì§ˆë¬¸


# -------------------------------
# Header
# -------------------------------
st.title("ğŸ¤– AI Agents íŠ¸ë Œë“œ Ã— ì§„ë¡œêµìœ¡(ì‚¬ê³  í™•ì¥)")
st.caption("CSV ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ë©°, â€˜ë‚˜ëŠ” ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í•˜ë‚˜ / ì–´ë–¤ ì¤€ë¹„ë¥¼ í•´ì•¼ í•˜ë‚˜â€™ë¥¼ í™•ì¥í•˜ê³  ì‚°ì¶œë¬¼ë¡œ ë‚¨ê¸°ëŠ” ì•±")
st.markdown("""
> **ëª©í‘œ:** íŠ¸ë Œë“œë¥¼ â€˜ì •ë³´â€™ë¡œ ëë‚´ì§€ ì•Šê³ ,  
> **ì—­í• (ROLE) â†’ ì—­ëŸ‰(SKILL) â†’ ê²½ë¡œ(PATH) â†’ ë‚˜ì˜ ì¤€ë¹„(PLAN)**ë¡œ ì—°ê²°í•´ ìŠ¤ìŠ¤ë¡œ ë‹µì„ ë§Œë“¤ì–´ë³´ê²Œ í•˜ê¸°
""")
st.divider()


# -------------------------------
# Data input + filters (Sidebar)
# -------------------------------
with st.sidebar:
    st.header("âš™ï¸ ë°ì´í„° ì„¤ì •")

    upload = st.file_uploader("CSV ì—…ë¡œë“œ(ì„ íƒ)", type=["csv"])
    if upload is not None:
        # ì—…ë¡œë“œ íŒŒì¼ ìš°ì„ 
        @st.cache_data(show_spinner=False)
        def load_uploaded(file) -> pd.DataFrame:
            # ì—…ë¡œë“œëŠ” bytesë¼ ì¸ì½”ë”© ì¶”ì •ì´ ê³¤ë€ â†’ pandasê°€ ì²˜ë¦¬í•˜ë˜ ì‹¤íŒ¨ì‹œ cp949
            try:
                dfu = pd.read_csv(file)
            except Exception:
                file.seek(0)
                dfu = pd.read_csv(file, encoding="cp949")
            return dfu

        raw = load_uploaded(upload)
        # ì„ì‹œ íŒŒì¼ ê²½ë¡œê°€ ì—†ìœ¼ë‹ˆ ë°”ë¡œ ì •ê·œí™”
        # (ê°„ë‹¨íˆ: dataframeì„ csvë¡œ ì €ì¥ í›„ load_dataì— ë„£ëŠ” ëŒ€ì‹ , load_data ë¡œì§ ì¼ë¶€ ì¬ì‚¬ìš©)
        df = raw.copy()
        cols = {c.strip().lower(): c for c in df.columns}
        need = ["title", "source", "date", "description", "link"]
        missing = [n for n in need if n not in cols]
        if missing:
            st.error(f"ì—…ë¡œë“œ CSV ì»¬ëŸ¼ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤: {', '.join(missing)}")
            st.stop()
        df = df.rename(columns={
            cols["title"]: "title",
            cols["source"]: "source",
            cols["date"]: "date",
            cols["description"]: "desc",
            cols["link"]: "link",
        })
        for c in ["title", "source", "desc", "link"]:
            df[c] = df[c].astype(str).fillna("").str.strip()
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df["domain"] = df["link"].str.extract(r"https?://([^/]+)", expand=False).fillna("")
        s = df["source"].str.lower()
        df["content_type"] = "news"
        df.loc[s.str.contains("arxiv"), "content_type"] = "paper"
        df.loc[s.str.contains("job"), "content_type"] = "job"

        def classify_role_local(title: str, desc: str, source: str) -> str:
            text = f"{title} {desc} {source}".lower()
            for role, patterns in ROLE_DEFS:
                for p in patterns:
                    if re.search(p, text):
                        return role
            if "arxiv" in text:
                return "ë¶„ì„ê°€(ë¦¬ì„œì¹˜/ë°ì´í„°)"
            return "ì„¤ê³„ì(ê¸°íš/êµ¬ì¡°í™”/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)"
        df["role"] = [classify_role_local(t, d, s) for t, d, s in zip(df["title"], df["desc"], df["source"])]

        df = df[(df["title"] != "") & (df["link"] != "")]
        df = df.drop_duplicates(subset=["link"]).reset_index(drop=True)
        if df["date"].notna().any():
            df["month"] = df["date"].dt.to_period("M").astype(str)
            df["week"] = df["date"].dt.to_period("W").astype(str)
        else:
            df["month"] = ""
            df["week"] = ""
    else:
        # ê¸°ë³¸ ê²½ë¡œ íŒŒì¼ ë¡œë”©
        try:
            df = load_data(DATA_PATH)
        except Exception as e:
            st.error("CSVë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.caption(str(e))
            st.markdown(f"- ê¸°ë³¸ ê²½ë¡œ: `{DATA_PATH}`")
            st.stop()

    st.caption(f"ë°ì´í„°: {len(df):,}ê°œ í•­ëª©")

    st.divider()
    st.header("ğŸ” í•„í„°")

    # date filter
    has_date = df["date"].notna().any()
    if has_date:
        min_d, max_d = df["date"].min().date(), df["date"].max().date()
        dr = st.date_input("ê¸°ê°„", value=(min_d, max_d))
        if isinstance(dr, tuple) and len(dr) == 2:
            start_d, end_d = dr
        else:
            start_d, end_d = min_d, max_d
    else:
        start_d = end_d = None
        st.info("Date íŒŒì‹±ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ê¸°ê°„ í•„í„°ê°€ ì œí•œë©ë‹ˆë‹¤.")

    sources_all = sorted(df["source"].unique().tolist())
    sources_sel = st.multiselect("Source", sources_all, default=sources_all)

    types_all = ["news", "paper", "job"]
    types_sel = st.multiselect("ì½˜í…ì¸  íƒ€ì…", types_all, default=types_all)

    # domain top 30
    dom_top = df["domain"].value_counts().head(30).index.tolist()
    dom_sel = st.multiselect("ë„ë©”ì¸(ìƒìœ„ 30)", dom_top, default=[])

    keyword = st.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰", placeholder="ì˜ˆ: evaluation, agentic, RAG, orchestration ...")

    st.divider()
    st.header("ğŸ§‘â€ğŸ« ìˆ˜ì—… ì˜µì…˜")
    audience = st.radio("ëŒ€ìƒ", ["ê³ 3", "ëŒ€í•™ìƒ"], horizontal=True)
    teacher_mode = st.toggle("êµì‚¬ìš© ê°€ì´ë“œ(ì§ˆë¬¸/í•´ì„¤) í‘œì‹œ", value=True)


# apply filters
f = df.copy()
if has_date and start_d and end_d:
    f = f[(f["date"].dt.date >= start_d) & (f["date"].dt.date <= end_d)]
f = f[f["source"].isin(sources_sel)]
f = f[f["content_type"].isin(types_sel)]
if dom_sel:
    f = f[f["domain"].isin(dom_sel)]
if keyword and keyword.strip():
    k = keyword.strip().lower()
    f = f[f["title"].str.lower().str.contains(k, na=False) | f["desc"].str.lower().str.contains(k, na=False)]


# -------------------------------
# Tabs
# -------------------------------
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "â‘  ì§€ê¸ˆì˜ ë³€í™”(Reality)",
    "â‘¡ í‚¤ì›Œë“œ(Topics)",
    "â‘¢ ì—­í•  ì§€ë„(Role Map)",
    "â‘£ ê²½ë¡œ ë¹„êµ(Path)",
    "â‘¤ ë‚˜ì˜ ì¤€ë¹„ ë¡œë“œë§µ(Plan)"
])


# ============================================================
# TAB 1: Reality Dashboard
# ============================================================
with tab1:
    st.subheader("ì§€ê¸ˆì˜ ë³€í™” í•œëˆˆì— ë³´ê¸°")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("í•„í„° í›„ í•­ëª©", f"{len(f):,}")
    c2.metric("Source ìˆ˜", f"{f['source'].nunique():,}")
    c3.metric("Domain ìˆ˜", f"{f['domain'].nunique():,}")
    if f["date"].notna().any():
        c4.metric("ìµœì‹  ë‚ ì§œ", str(f["date"].max().date()))
    else:
        c4.metric("ìµœì‹  ë‚ ì§œ", "-")

    st.divider()

    left, right = st.columns(2)
    with left:
        src = f["source"].value_counts().reset_index()
        src.columns = ["source", "count"]
        fig = px.bar(src.head(12), x="source", y="count", title="Source ë¶„í¬(ìƒìœ„ 12)")
        st.plotly_chart(fig, use_container_width=True)

    with right:
        ct = f["content_type"].value_counts().reset_index()
        ct.columns = ["content_type", "count"]
        fig = px.pie(ct, names="content_type", values="count", title="ì½˜í…ì¸  íƒ€ì… ë¹„ì¤‘")
        st.plotly_chart(fig, use_container_width=True)

    if f["domain"].notna().any():
        dom = f["domain"].value_counts().head(15).reset_index()
        dom.columns = ["domain", "count"]
        fig = px.bar(dom, x="domain", y="count", title="Domain Top 15(ì§€ì‹/ê¸°íšŒê°€ ìƒê¸°ëŠ” ê³³)")
        st.plotly_chart(fig, use_container_width=True)

    if f["date"].notna().any():
        st.divider()
        st.subheader("ê¸°ê°„ë³„ íë¦„(ì£¼ ë‹¨ìœ„)")
        w = f.dropna(subset=["date"]).groupby(["week", "content_type"]).size().reset_index(name="count")
        fig = px.line(w, x="week", y="count", color="content_type", markers=True, title="ì£¼ë³„ ë“±ì¥ ì¶”ì„¸")
        st.plotly_chart(fig, use_container_width=True)

    st.info("ğŸ’¡ ì´ í™”ë©´ì˜ ëª©ì : â€˜ì§€ê¸ˆ ë³€í™”ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•œë‹¤â€™ë¥¼ ë°ì´í„°ë¡œ ì²´ê°í•˜ê¸°")
    if teacher_mode:
        with st.expander("ğŸ‘©â€ğŸ« êµì‚¬ìš© ì§ˆë¬¸(ì‚¬ê³  í™•ì¥)"):
            st.markdown("""
- ì—°êµ¬(ë…¼ë¬¸) / ì‚°ì—…(ë‰´ìŠ¤Â·íˆ´) / ì±„ìš©(ì¼ìë¦¬) ì¤‘ **ì–´ë””ê°€ ë¨¼ì € ì›€ì§ì´ëŠ” ëŠë‚Œ**ì¸ê°€?
- ì‚¬ëŒë“¤ì´ ì •ë³´ë¥¼ ì–»ëŠ” ê³³(ë„ë©”ì¸)ì´ í•œìª½ìœ¼ë¡œ ëª°ë ¤ ìˆë‹¤ë©´, ê·¸ê²Œ ì˜ë¯¸í•˜ëŠ” ê²ƒì€?
- â€œë³€í™”ê°€ ë¹ ë¥´ë‹¤â€ëŠ” ê±´ ê²°êµ­ **ì–´ë–¤ ëŠ¥ë ¥ì„ ìš”êµ¬**í•˜ëŠ”ê°€?
""")


# ============================================================
# TAB 2: Topics (Keywords)
# ============================================================
with tab2:
    st.subheader("í‚¤ì›Œë“œë¡œ ë³´ëŠ” â€˜ì¼ì˜ ì¤‘ì‹¬ì¶•â€™ ë³€í™”")
    st.caption("Title+Descriptionì—ì„œ ì˜ë¬¸ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ â€˜ë¬´ì—‡ì´ ë°˜ë³µì ìœ¼ë¡œ ë“±ì¥í•˜ëŠ”ê°€â€™ë¥¼ ë³¸ë‹¤(ê°„ë‹¨ ë£° ê¸°ë°˜).")

    colA, colB = st.columns(2)

    with colA:
        kw = top_keywords(f, n=25)
        if kw:
            kw_df = pd.DataFrame(kw, columns=["keyword", "count"])
            fig = px.bar(kw_df, x="keyword", y="count", title="í‚¤ì›Œë“œ Top 25(í•„í„° ê¸°ì¤€)")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í•„í„°ë¥¼ ì™„í™”í•´ë³´ì„¸ìš”.")

    with colB:
        if df["date"].notna().any():
            rising = rising_keywords(df, recent_days=30, n=15)
            if rising:
                r_df = pd.DataFrame(rising, columns=["keyword", "score", "recent_count", "all_count"])
                fig = px.bar(r_df, x="keyword", y="score", title="ìµœê·¼ 30ì¼ â€˜ìƒìŠ¹â€™ í‚¤ì›Œë“œ(ê°„ë‹¨ ì¦ê° ì ìˆ˜)")
                st.plotly_chart(fig, use_container_width=True)
                st.caption("ì ìˆ˜ëŠ” â€˜ìµœê·¼ ë¹„ì¤‘ - ì „ì²´ ë¹„ì¤‘â€™ìœ¼ë¡œ ê³„ì‚°(ì •êµí•œ íŠ¸ë Œë”©ì´ ì•„ë‹ˆë¼ ìˆ˜ì—…ìš© ì‹ í˜¸).")
            else:
                st.info("ìµœê·¼ 30ì¼ ë¹„êµë¥¼ í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤(ë‚ ì§œ/ê¸°ê°„ í™•ì¸).")
        else:
            st.info("Dateê°€ ì—†ì–´ ìƒìŠ¹ í‚¤ì›Œë“œ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤.")

    st.divider()
    st.markdown("### ğŸ§  í•™ìƒ ì‚¬ê³  í™•ì¥ ì§ˆë¬¸(í™”ë©´ì— ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥)")
    st.markdown("""
- ì§€ê¸ˆ ë°˜ë³µì ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œëŠ” **â€˜ê¸°ìˆ â€™**ì¸ê°€, **â€˜ì¼ì˜ ë°©ì‹/ê·œì¹™â€™**ì¸ê°€?  
- ì´ í‚¤ì›Œë“œëŠ” ì‚¬ëŒì˜ ì¼ì„ **ì¤„ì´ëŠ”ê°€ / ë°”ê¾¸ëŠ”ê°€ / ìƒˆë¡œ ë§Œë“œëŠ”ê°€**?  
- ë‚´ê°€ ì´ í‚¤ì›Œë“œë¥¼ â€˜ìˆ˜ì—… ê³¼ì œâ€™ë¡œ ë°”ê¾¼ë‹¤ë©´, **10ë¶„ì§œë¦¬ í–‰ë™**ì€ ë¬´ì—‡ì¼ê¹Œ?
""")
    if teacher_mode:
        with st.expander("ğŸ‘©â€ğŸ« êµì‚¬ìš© ìš´ì˜ íŒ"):
            st.markdown("""
- í‚¤ì›Œë“œëŠ” â€˜ìœ í–‰â€™ì´ ì•„ë‹ˆë¼ **ì•ìœ¼ë¡œ ìì£¼ ë§Œë‚˜ê²Œ ë  ë¬¸ì œì˜ ì´ë¦„**ìœ¼ë¡œ í•´ì„í•˜ê²Œ ë•ìŠµë‹ˆë‹¤.
- í•™ìƒì´ â€˜ëª¨ë¥¸ë‹¤â€™ê³  í•˜ë©´ ì •ìƒì…ë‹ˆë‹¤. ëŒ€ì‹  **ëª¨ë¥´ëŠ” ê²ƒì„ ë‹¤ë£¨ëŠ” ë°©ì‹(ì§ˆë¬¸ ë§Œë“¤ê¸°)**ì„ í•™ìŠµëª©í‘œë¡œ ì¡ìœ¼ì„¸ìš”.
""")


# ============================================================
# TAB 3: Role Map (Roles)
# ============================================================
with tab3:
    st.subheader("â€˜ì§ì—…ëª…â€™ì´ ì•„ë‹ˆë¼ â€˜ì—­í• (ROLE)â€™ë¡œ ë³´ê¸°")
    st.caption("CSV í…ìŠ¤íŠ¸ë¥¼ ê°„ë‹¨í•œ ê·œì¹™ìœ¼ë¡œ ì—­í• ë¡œ ë¶„ë¥˜í•˜ê³ , í•™ìƒì€ â€˜ë‚˜ëŠ” ì–´ë–¤ ì—­í• ì— ëŒë¦¬ëŠ”ê°€â€™ë¥¼ ì„ íƒí•œë‹¤.")

    left, right = st.columns(2)
    with left:
        role_dist = f["role"].value_counts().reset_index()
        role_dist.columns = ["role", "count"]
        fig = px.bar(role_dist, x="role", y="count", title="ì—­í•  ë¶„í¬(í•„í„° ê¸°ì¤€)")
        st.plotly_chart(fig, use_container_width=True)

    with right:
        if f["date"].notna().any():
            cutoff = pd.Timestamp(date.today() - timedelta(days=30))
            recent = f[f["date"] >= cutoff]
            comp = pd.DataFrame({
                "ì „ì²´": f["role"].value_counts(),
                "ìµœê·¼30ì¼": recent["role"].value_counts()
            }).fillna(0).astype(int).reset_index().rename(columns={"index":"role"})
            comp_melt = comp.melt(id_vars=["role"], var_name="range", value_name="count")
            fig = px.bar(comp_melt, x="role", y="count", color="range", barmode="group",
                         title="ì—­í•  ë¹„êµ: ì „ì²´ vs ìµœê·¼ 30ì¼")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Dateê°€ ì—†ì–´ ìµœê·¼ 30ì¼ ì—­í•  ë¹„êµê°€ ì œí•œë©ë‹ˆë‹¤.")

    st.divider()
    st.markdown("### ğŸ¯ í•™ìƒ í™œë™: â€˜ë‚˜ì˜ ì—­í•  Top 2â€™ ì„ íƒí•˜ê¸°")

    role_choices = list(role_dist["role"].tolist()) if len(role_dist) else [r[0] for r in ROLE_DEFS]
    sel_roles = st.multiselect("ëŒë¦¬ëŠ” ì—­í• ì„ 2ê°œ ì„ íƒ(ê¶Œì¥)", role_choices, default=role_choices[:2])

    reason_like = st.text_area("ì™œ ëŒë¦¬ë‚˜ìš”? (ì´ìœ  1~2ì¤„)", placeholder="ì˜ˆ: ë¬¸ì œë¥¼ êµ¬ì¡°í™”í•˜ê³  ë°©í–¥ì„ ì •í•˜ëŠ” ì¼ì´ ì¬ë°Œì„ ê²ƒ ê°™ì•„ì„œ")
    reason_hard = st.text_area("ë¬´ì—‡ì´ ë¶€ë‹´/ì–´ë ¤ì›€ìœ¼ë¡œ ëŠê»´ì§€ë‚˜ìš”? (1~2ì¤„)", placeholder="ì˜ˆ: ê¸°ìˆ  ìš©ì–´ê°€ ë‚¯ì„¤ê³  ì‹œì‘ì´ ë§‰ë§‰í•¨")

    # pick an item to anchor
    st.markdown("### ğŸ§· â€˜ì—­í• â€™ì´ ì‹¤ì œë¡œ ë³´ì´ëŠ” ì‚¬ë¡€ 1ê°œ ê³ ë¥´ê¸°")
    f_sel = f.sort_values("date", ascending=False).head(400) if len(f) > 400 else f
    if len(f_sel) == 0:
        st.info("í•„í„° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì™¼ìª½ í•„í„°ë¥¼ ì™„í™”í•´ë³´ì„¸ìš”.")
    else:
        chosen = st.selectbox("ì‚¬ë¡€ ì„ íƒ(ìµœê·¼ìˆœ ìƒìœ„ 400ê°œ)", f_sel.apply(item_label, axis=1).tolist())
        idx = f_sel.apply(item_label, axis=1).tolist().index(chosen)
        row = f_sel.iloc[idx]

        st.markdown(f"**{row['title']}**")
        st.caption(f"{row['source']} Â· {row['content_type']} Â· {row['domain']} Â· "
                   f"{row['date'].date() if pd.notna(row['date']) else ''} Â· ë¶„ë¥˜ì—­í• : {row['role']}")
        st.write(row["desc"][:900] + ("â€¦" if len(row["desc"]) > 900 else ""))
        st.link_button("ì›ë¬¸ ë³´ê¸°", row["link"])

        st.markdown("#### ğŸ”½ ì´ ì‚¬ë¡€ë¥¼ â€˜ë‚˜ì˜ ê¸°ë¡â€™ì— ì¶”ê°€")
        my_role = st.selectbox("ë‚´ê°€ ë³´ê¸°ì—” ì´ ì‚¬ë¡€ì˜ í•µì‹¬ ì—­í• ì€?", [r[0] for r in ROLE_DEFS], index=0)
        my_one_line = st.text_input("í•œ ì¤„ í•´ì„(ë‚´ ì–¸ì–´ë¡œ)", placeholder="ì˜ˆ: ì‚¬ëŒì€ ê²°êµ­ â€˜ê²€ì¦ ê¸°ì¤€â€™ì„ ë§Œë“¤ê³  ë°˜ë³µ ì‹¤í—˜ì„ ì„¤ê³„í•œë‹¤")
        my_next_10 = st.text_input("10ë¶„ í–‰ë™(ì§€ê¸ˆ ë‹¹ì¥)", placeholder="ì˜ˆ: ëª¨ë¥´ëŠ” ìš©ì–´ 3ê°œ ì •ì˜ ì°¾ì•„ ë©”ëª¨í•˜ê¸°")

        if st.button("ğŸ“Œ ê¸°ë¡ ì¶”ê°€", use_container_width=True):
            st.session_state["portfolio"].append({
                "date": str(row["date"].date()) if pd.notna(row["date"]) else "",
                "title": row["title"],
                "source": row["source"],
                "content_type": row["content_type"],
                "domain": row["domain"],
                "link": row["link"],
                "role_auto": row["role"],
                "role_mine": my_role,
                "why_like": reason_like.strip(),
                "why_hard": reason_hard.strip(),
                "one_line": my_one_line.strip(),
                "next10": my_next_10.strip(),
                "my_roles_top2": ", ".join(sel_roles[:2]) if sel_roles else ""
            })
            st.success("ê¸°ë¡ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. â‘¤ â€˜ë‚˜ì˜ ì¤€ë¹„ ë¡œë“œë§µâ€™ì—ì„œ ìë™ ì •ë¦¬ë©ë‹ˆë‹¤.")

    if teacher_mode:
        with st.expander("ğŸ‘©â€ğŸ« êµì‚¬ìš© ì§ˆë¬¸(í•µì‹¬)"):
            st.markdown("""
- â€œì´ í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ëŒì´ í•˜ëŠ” ì¼ì€ â€˜ë¬´ì—‡ì„ ê²°ì •/ì„¤ê³„/êµ¬í˜„/ìš´ì˜/ê²€ì¦/ì„¤ëª…â€™í•˜ëŠ” ê²ƒì¸ê°€?â€
- â€œë‚˜ëŠ” ì–´ë–¤ ì—­í• ì´ ë” ìì—°ìŠ¤ëŸ¬ìš´ê°€? ê·¸ ì´ìœ ëŠ” â€˜í¥ë¯¸/ê°€ì¹˜/ê°•ì â€™ ì¤‘ ë¬´ì—‡ì¸ê°€?â€
- â€œë¶€ë‹´(ì–´ë ¤ì›€)ì€ ì•½ì  ê³ ë°±ì´ ì•„ë‹ˆë¼ **ì¤€ë¹„ ê³¼ì œ**ë‹¤. ë¬´ì—‡ì„ ì¤€ë¹„í•˜ë©´ ë°”ë€”ê¹Œ?â€
""")


# ============================================================
# TAB 4: Path comparison (Triad)
# ============================================================
with tab4:
    st.subheader("ê°™ì€ ì£¼ì œ, ë‹¤ë¥¸ ê²½ë¡œ: ë…¼ë¬¸â€“ì‹¤ë¬´â€“ì±„ìš©")
    st.caption("ê°™ì€ í‚¤ì›Œë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ â€˜ì—°êµ¬/ì‹¤ë¬´/ì±„ìš©â€™ 3ì¢… ì„¸íŠ¸ë¥¼ ë‚˜ë€íˆ ë³´ê³  ê²½ë¡œ ë‹¤ì–‘ì„±ì„ ì²´ê°í•œë‹¤.")

    if len(f) == 0:
        st.info("í•„í„° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì™¼ìª½ í•„í„°ë¥¼ ì™„í™”í•´ë³´ì„¸ìš”.")
    else:
        base = st.selectbox("ê¸°ì¤€ ì•„ì´í…œ ì„ íƒ(ìµœê·¼ìˆœ ìƒìœ„ 400ê°œ)", (f.sort_values("date", ascending=False).head(400)).apply(item_label, axis=1).tolist())
        f_base = f.sort_values("date", ascending=False).head(400)
        idx = f_base.apply(item_label, axis=1).tolist().index(base)
        base_row = f_base.iloc[idx]

        # ìë™ í‚¤ì›Œë“œ ì œì•ˆ
        auto_keys = Counter(tokenize_en(base_row["title"] + " " + base_row["desc"])).most_common(10)
        suggested = auto_keys[0][0] if auto_keys else ""
        key = st.text_input("í‚¤ì›Œë“œ(ìë™ ì œì•ˆ â†’ ìˆ˜ì • ê°€ëŠ¥)", value=suggested)

        if key.strip():
            picks, sub = triad_pick(f, key)
            cols = st.columns(3)
            mapping = {"paper": "ë…¼ë¬¸(Research)", "news": "ì‚°ì—…/ë„êµ¬(Practice)", "job": "ì±„ìš©(Job)"}

            for i, t in enumerate(["paper", "news", "job"]):
                with cols[i]:
                    st.markdown(f"### {mapping[t]}")
                    if t in picks:
                        r = picks[t]
                        st.markdown(f"**{r['title']}**")
                        st.caption(f"{r['source']} Â· {r['date'].date() if pd.notna(r['date']) else ''} Â· {r['domain']} Â· ì—­í• : {r['role']}")
                        st.write(r["desc"][:260] + ("â€¦" if len(r["desc"]) > 260 else ""))
                        st.link_button("ì›ë¬¸ ë³´ê¸°", r["link"])
                    else:
                        st.info("í•´ë‹¹ ìœ í˜•ì—ì„œ ë§¤ì¹­ë˜ëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

            st.divider()
            st.markdown("### ğŸ§  í•™ìƒ ì‚¬ê³  í™•ì¥ ì§ˆë¬¸")
            st.markdown("""
- ê°™ì€ ì£¼ì œì¸ë°ë„ **ë…¼ë¬¸/ì‹¤ë¬´/ì±„ìš©**ì´ ê°•ì¡°í•˜ëŠ” í¬ì¸íŠ¸ê°€ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€?  
- ë‚˜ëŠ” ì´ ì£¼ì œë¥¼ ì–´ë–¤ ê²½ë¡œë¡œ ë‹¤ë£¨ê³  ì‹¶ì€ê°€? (ì—°êµ¬/êµ¬í˜„/ìš´ì˜/í‰ê°€/ì„¤ëª…)  
- â€˜ì „ê³µâ€™ì€ ê³ ì •ì´ ì•„ë‹ˆë¼, **ê²½ë¡œì— ë§ê²Œ ì¬êµ¬ì„±ë˜ëŠ” ì¤€ë¹„**ë‹¤. ë‚´ ê²½ë¡œëŠ”?
""")

            if st.button("ğŸ“Œ ì´ ì£¼ì œ(í‚¤ì›Œë“œ)ë¥¼ ê¸°ë¡ì— ì¶”ê°€", use_container_width=True):
                st.session_state["notes"].append({
                    "topic_keyword": key.strip(),
                    "base_title": base_row["title"],
                    "note": ""
                })
                st.success("ì£¼ì œ ë©”ëª¨ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. â‘¤ì—ì„œ í•¨ê»˜ ì •ë¦¬í•  ìˆ˜ ìˆì–´ìš”.")
        else:
            st.info("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if teacher_mode:
        with st.expander("ğŸ‘©â€ğŸ« êµì‚¬ìš© ìš´ì˜ íŒ"):
            st.markdown("""
- í•™ìƒë“¤ì´ â€˜ì „ê³µ=ì§ì—…â€™ìœ¼ë¡œ ë‹¨ì„ ì ìœ¼ë¡œ ìƒê°í•  ë•Œ, ì´ í™”ë©´ì´ ì „í™˜ì ì„ ë§Œë“­ë‹ˆë‹¤.
- ê°™ì€ í‚¤ì›Œë“œë¼ë„ ê²½ë¡œë§ˆë‹¤ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìƒê¹ë‹ˆë‹¤:
  - ë…¼ë¬¸: â€œë¬´ì—‡ì´ ì‚¬ì‹¤ì¸ê°€?â€
  - ì‹¤ë¬´: â€œì–´ë–»ê²Œ êµ¬í˜„/ì ìš©í•˜ëŠ”ê°€?â€
  - ì±„ìš©: â€œì–´ë–¤ ì—­í• ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ê°€?â€
""")


# ============================================================
# TAB 5: My Plan (Roadmap output)
# ============================================================
with tab5:
    st.subheader("ë‚˜ì˜ ì¤€ë¹„ ë¡œë“œë§µ(ì‚°ì¶œë¬¼)")
    st.caption("ì„ íƒ/ê¸°ë¡ì„ ìë™ ì •ë¦¬í•˜ê³ , â€˜ì´ë²ˆ ì£¼ 10ë¶„ í–‰ë™ + ì´ë²ˆ ë‹¬ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸â€™ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.")

    items = st.session_state.get("portfolio", [])
    notes = st.session_state.get("notes", [])

    if not items and not notes:
        st.info("ì•„ì§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. â‘¢/â‘£ íƒ­ì—ì„œ ì‚¬ë¡€ ë˜ëŠ” ì£¼ì œë¥¼ ê¸°ë¡í•´ë³´ì„¸ìš”.")
    else:
        if items:
            p = pd.DataFrame(items)
            st.markdown("### 1) ì˜¤ëŠ˜ ë‚´ê°€ ë§Œë“  ê¸°ë¡")
            st.dataframe(p, use_container_width=True, hide_index=True)

            st.divider()
            st.markdown("### 2) ë‚˜ì˜ ê´€ì‹¬ ì—­í• (Top)")
            if "role_mine" in p.columns:
                role_counts = p["role_mine"].value_counts().reset_index()
                role_counts.columns = ["role", "count"]
                fig = px.bar(role_counts, x="role", y="count", title="ë‚´ê°€ ì„ íƒí•œ ì—­í•  ë¶„í¬")
                st.plotly_chart(fig, use_container_width=True)

            # ì¶”ì²œ ì—­ëŸ‰ ìë™ ì œì•ˆ
            st.divider()
            st.markdown("### 3) ì—­í•  ê¸°ë°˜ â€˜ì¶”ì²œ ì—­ëŸ‰â€™(ìë™ ì œì•ˆ)")

            top_role = None
            if "role_mine" in p.columns and p["role_mine"].notna().any():
                top_role = p["role_mine"].value_counts().index[0]

            if top_role and top_role in ROLE_TO_SKILLS:
                sugg = ROLE_TO_SKILLS[top_role]
                st.success(f"ê°€ì¥ ë§ì´ ì„ íƒëœ ì—­í• : **{top_role}**")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown("**ê¸°ìˆ (Tech)**")
                    st.write(", ".join(sugg["tech"]))
                with col2:
                    st.markdown("**ì¸ì§€(Cognition)**")
                    st.write(", ".join(sugg["cog"]))
                with col3:
                    st.markdown("**íƒœë„(Attitude)**")
                    st.write(", ".join(sugg["att"]))
            else:
                st.info("ì—­ëŸ‰ ìë™ ì œì•ˆì€ â€˜ë‚´ê°€ ì„ íƒí•œ ì—­í• â€™ì´ ìˆì„ ë•Œ ë” ì •í™•í•´ì§‘ë‹ˆë‹¤.")

            st.divider()
            st.markdown("### 4) ë‚˜ì˜ ì¤€ë¹„ ì²´í¬(ì„ íƒ/ê¸°ë¡)")
            c1, c2, c3 = st.columns(3)
            with c1:
                tech = st.multiselect("ê¸°ìˆ  ì—­ëŸ‰(ì²´í¬)", SKILL_TECH, default=[])
            with c2:
                cog = st.multiselect("ì¸ì§€ ì—­ëŸ‰(ì²´í¬)", SKILL_COG, default=[])
            with c3:
                att = st.multiselect("íƒœë„ ì—­ëŸ‰(ì²´í¬)", SKILL_ATT, default=[])

            st.markdown("### 5) â€˜ë‚˜ëŠ” ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í•˜ë‚˜?â€™ë¥¼ â€˜ì¤€ë¹„ ê³„íšâ€™ìœ¼ë¡œ ë°”ê¾¸ê¸°")
            if audience == "ê³ 3":
                st.markdown("""
- **í•µì‹¬:** ì „ê³µ í™•ì •ì´ ì•„ë‹ˆë¼, **ì—­í• ê³¼ í•™ìŠµ ìŠµê´€**ì„ ë§Œë“ ë‹¤  
- **ì¢‹ì€ ê³„íš:** â€œì‘ê²Œ í•´ë³´ê³  â†’ ê¸°ë¡í•˜ê³  â†’ ì§ˆë¬¸ì„ ì—…ë°ì´íŠ¸â€  
""")
                next10_tpl = "ì˜ˆ: ëª¨ë¥´ëŠ” ìš©ì–´ 3ê°œ ì •ì˜ ì°¾ì•„ì„œ ë…¸íŠ¸ì— ì •ë¦¬í•˜ê¸°"
                month_tpl = "ì˜ˆ: ê´€ì‹¬ í‚¤ì›Œë“œ 1ê°œë¡œ ë¯¸ë‹ˆ ë°œí‘œìë£Œ 1ì¥ ë§Œë“¤ê¸°(ì—­í• /ì—­ëŸ‰/ê²½ë¡œ ì •ë¦¬)"
            else:
                st.markdown("""
- **í•µì‹¬:** ì „ê³µê³¼ ë¬´ê´€í•˜ê²Œ, **ê²½ë¡œ(ì—°êµ¬/ì‹¤ë¬´/ì±„ìš©) ì¤‘ ì–´ë””ë¡œ ê°ˆì§€**ë¥¼ ì •í•˜ê³  ì¤€ë¹„ë¥¼ ìŒ“ëŠ”ë‹¤  
- **ì¢‹ì€ ê³„íš:** â€œì‘ì€ í”„ë¡œì íŠ¸ â†’ í¬íŠ¸í´ë¦¬ì˜¤ â†’ í”¼ë“œë°±(ë©˜í† /ë™ë£Œ)â€  
""")
                next10_tpl = "ì˜ˆ: ê´€ì‹¬ ì£¼ì œ ê´€ë ¨ ê¸€ 1ê°œ ì½ê³  â€˜ì—­í• /ì—­ëŸ‰/ê²½ë¡œâ€™ 3ì¤„ ìš”ì•½"
                month_tpl = "ì˜ˆ: Streamlit/ë…¸ì…˜/ê¹ƒí—ˆë¸Œë¡œ â€˜íŠ¸ë Œë“œâ†’ì—­í• â†’ì—­ëŸ‰â€™ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ ì œì‘"

            next10 = st.text_input("ì´ë²ˆ ì£¼ 10ë¶„ í–‰ë™ 1ê°œ", value=next10_tpl)
            month_project = st.text_input("ì´ë²ˆ ë‹¬ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ 1ê°œ", value=month_tpl)
            help_people = st.text_input("ë„ì›€ ë°›ì„ ìì›/ì‚¬ëŒ(1ê°œ)", placeholder="ì˜ˆ: ë‹´ì„/ì§„ë¡œìŒ¤, ì„ ë°°, ì»¤ë®¤ë‹ˆí‹°, ìœ íŠœë¸Œ ê°•ì˜, í•™êµ ë™ì•„ë¦¬")

            one_line = st.text_area("ë‚˜ì˜ í•œ ì¤„ ì„ ì–¸ë¬¸", placeholder="ì˜ˆ: ë‚˜ëŠ” í‰ê°€ì ì—­í• ì— ëŒë¦¬ê³ , ì´ë²ˆ ë‹¬ì—ëŠ” ê²€ì¦ ê¸°ì¤€ì„ ë§Œë“œëŠ” ì—°ìŠµì„ ì‹œì‘í•˜ê² ë‹¤.")

            st.divider()
            st.markdown("### 6) ì‚°ì¶œë¬¼ ë¯¸ë¦¬ë³´ê¸°(ë³µì‚¬í•´ì„œ ì œì¶œ ê°€ëŠ¥)")
            top2 = p["my_roles_top2"].dropna().iloc[-1] if "my_roles_top2" in p.columns and len(p["my_roles_top2"].dropna()) else ""
            sample_line = p["one_line"].dropna().iloc[-1] if "one_line" in p.columns and len(p["one_line"].dropna()) else ""
            picked_title = p["title"].iloc[-1] if len(p) else ""
            picked_role = p["role_mine"].iloc[-1] if "role_mine" in p.columns and len(p) else ""

            st.code(
f"""[ì˜¤ëŠ˜ì˜ ì§„ë¡œ ì‚¬ê³  í™•ì¥ ê¸°ë¡]

- ë‚´ê°€ ë³¸ ë³€í™”(ì‚¬ë¡€): {picked_title}
- ë‚´ê°€ í•´ì„í•œ í•µì‹¬ ì—­í• : {picked_role}
- ëŒë¦¬ëŠ” ì—­í•  Top2: {top2}
- í•œ ì¤„ í•´ì„: {sample_line}

[ë‚˜ì˜ ì¤€ë¹„ ê³„íš]

- ê¸°ìˆ  ì—­ëŸ‰(ì²´í¬): {", ".join(tech)}
- ì¸ì§€ ì—­ëŸ‰(ì²´í¬): {", ".join(cog)}
- íƒœë„ ì—­ëŸ‰(ì²´í¬): {", ".join(att)}
- ì´ë²ˆ ì£¼ 10ë¶„ í–‰ë™: {next10}
- ì´ë²ˆ ë‹¬ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: {month_project}
- ë„ì›€ ë°›ì„ ìì›/ì‚¬ëŒ: {help_people}
- ë‚˜ì˜ í•œ ì¤„ ì„ ì–¸ë¬¸: {one_line}
""",
                language="text"
            )

            colD1, colD2 = st.columns(2)
            with colD1:
                csv_bytes = p.to_csv(index=False).encode("utf-8-sig")
                st.download_button("â¬‡ï¸ ê¸°ë¡ CSV ë‹¤ìš´ë¡œë“œ", csv_bytes,
                                   file_name=f"career_portfolio_{date.today().isoformat()}.csv",
                                   mime="text/csv", use_container_width=True)
            with colD2:
                if st.button("ğŸ—‘ï¸ ê¸°ë¡ ì „ì²´ ì‚­ì œ", use_container_width=True):
                    st.session_state["portfolio"] = []
                    st.session_state["notes"] = []
                    st.success("ê¸°ë¡ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤. (í•„ìš”í•˜ë©´ ìƒˆë¡œê³ ì¹¨)")

        if notes:
            st.divider()
            st.markdown("### 7) ì£¼ì œ ë©”ëª¨(í‚¤ì›Œë“œ) ëª¨ì•„ë³´ê¸°")
            ndf = pd.DataFrame(notes)
            st.dataframe(ndf, use_container_width=True, hide_index=True)


# ============================================================
# Footer: Student questions (optional, keeps your original intent)
# ============================================================
st.divider()
st.header("â“ ìµëª… ì§ˆë¬¸ ìˆ˜ì§‘(ìˆ˜ì—…ìš©)")

qcol1, qcol2 = st.columns([1, 1])
with qcol1:
    q = st.text_area("ì§ˆë¬¸ì„ ì ì–´ì£¼ì„¸ìš”", placeholder="ì˜ˆ: ë¬¸ê³¼ë„ AI ê´€ë ¨ ì§„ë¡œê°€ ê°€ëŠ¥í• ê¹Œìš”?")
    if st.button("ğŸ“¥ ì§ˆë¬¸ ì œì¶œ", use_container_width=True):
        if q.strip():
            st.session_state["student_questions"].append(q.strip())
            st.success("ì§ˆë¬¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

def classify_question(text: str) -> str:
    t = str(text).lower()
    if re.search(r"ì „ê³µ|í•™ê³¼|ê³¼|ì„ íƒ|í¸ì…|ë³µìˆ˜|ë¶€ì „ê³µ", t):
        return "ì „ê³µ/í•™ê³¼"
    if re.search(r"ê³µë¶€|ì—­ëŸ‰|ì¤€ë¹„|ìˆ˜í•™|ì½”ë”©|ìê²©|í¬íŠ¸í´ë¦¬ì˜¤", t):
        return "ì—­ëŸ‰/ì¤€ë¹„"
    if re.search(r"ì§ì—…|ì·¨ì—…|ì¼ìë¦¬|ì»¤ë¦¬ì–´|ì—°ë´‰|íšŒì‚¬", t):
        return "ì§„ë¡œ/ì§ì—…"
    if re.search(r"ë¶ˆì•ˆ|ê±±ì •|ë‘ë ¤|ëª»í• |ê´œì°®", t):
        return "ë¶ˆì•ˆ/ê³ ë¯¼"
    return "ê¸°íƒ€"

with qcol2:
    if st.session_state["student_questions"]:
        q_df = pd.DataFrame({
            "question": st.session_state["student_questions"],
            "type": [classify_question(x) for x in st.session_state["student_questions"]]
        })
        dist = q_df["type"].value_counts().reset_index()
        dist.columns = ["type", "count"]
        fig = px.bar(dist, x="type", y="count", title="ì§ˆë¬¸ ìœ í˜• ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("ì•„ì§ ìˆ˜ì§‘ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
